{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42b17b62",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Initialize Otter\n",
    "import otter\n",
    "grader = otter.Notebook(\"lab10.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee70a7c1-d20d-4b63-9365-c915a0f76203",
   "metadata": {},
   "source": [
    "# Lab 10: Bandits \n",
    "Welcome to the tenth DS102 lab! \n",
    "\n",
    "The goals of this lab is to implement and gain a better understanding of the pros and cons of the Upper Confidence Bounds (UCB) and Thompson Sampling algorithms for the multi-armed bandits problem.\n",
    "\n",
    "The code you need to write is indicated by `...`. There is additional documentation for each part as you go along.\n",
    "\n",
    "\n",
    "## Collaboration Policy\n",
    "You can submit the lab in pairs (groups of two, no more than two). **If you choose to work in a pair, please make sure to add your group member on Gradescope for both written and code submission.**\n",
    "\n",
    "Data science is a collaborative activity. While you may talk with others about the labs, we ask that you **write your solutions individually and do not share your code with anyone other than your partner**. If you do discuss the assignments with people other than your partner please **include their names** in the cell below.\n",
    "\n",
    "`<Collaborator Name> <Collaborator e-mail>`\n",
    "\n",
    "## Submission\n",
    "**For full credit, this assignment should be completed and submitted before Wednesday, Apr 9th, 2025 at 05:00 PM PST.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4449f7f1-2409-4caa-a23c-97aef44d3e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import seaborn as sns\n",
    "from matplotlib.widgets import Button, CheckButtons\n",
    "from matplotlib import gridspec\n",
    "import functools\n",
    "from Bandit_env import BanditEnv, Interactive_UCB_Algorithm,Interactive_TS_Algorithm\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df2a797-16db-42f5-b6f5-c0dbad4bea0d",
   "metadata": {},
   "source": [
    "# Multi-Armed Bandits (MAB)\n",
    "\n",
    "In this lab we will be implementing two of the most common approaches to solving stochastic Multi-Armed Bandit (MAB) problems. We first define the problem and then you will have a chance to implement the Upper Confidence Bound (UCB) algorithm and the Thompson Sampling (TS) algorithm from lecture and analyze their performance.\n",
    "\n",
    "Recall that MABs are useful when we are making sequential decisions with feedback: each decision is between several choices (which we'll call arms), and when we make a decision, we get some feedback that we'll use to make good decisions in the future.\n",
    "\n",
    "## Problem Setting: \n",
    "\n",
    "A MAB problem is a simple setting in which it is easy to analyze the __exploration/exploitation__ tradeoff that is extremely common in online machine learning. The setup is as follows:\n",
    "\n",
    "A MAB instance is a set $\\mathcal{A}$ of $K$ arms (these represent different choices that we can make). Each arm $a \\in \\mathcal{A}$ is associated with a reward distribution $X_a \\sim p_a$ which is unique to that arm. The mean of an arm $a \\in \\mathcal{A}$ is denoted as $\\mu_a=\\mathbb{E}[X_a]$. \n",
    "\n",
    "At each time $t=1,2,...$, an algorithm that is interacting with a MAB must choose an arm, $A_t \\in \\mathcal{A}$. The algorithm then receives a reward $X_{A_t}^{(t)}$ sampled independently from $\\mathbb{P}_{A_t}$.\n",
    "\n",
    "The __goal__ of an algorithm interacting with a MAB instance is to find the arm $A^\\ast$ with the highest mean reward $\\mu^\\ast$ as fast as possible while maintaining performance. That is, it would like to find the single optimal arm $A^\\ast$ such that:\n",
    "\n",
    "$$ A^\\ast=arg\\max_{a \\in \\mathcal{A}} \\mu_a$$\n",
    "\n",
    "where $\\mu^*=\\mu_{A^\\ast}$.\n",
    "\n",
    "This is often encoded as wanting to find an algorithm that minimizes the __pseudoregret__ over a time horizon $T$. Intuitively, the pseudoregret is the best the algorithm could have done in hindsight, **on average**, if it had known which was the optimal arm ($A^*$). In order to simplify our analysis and focus on getting to the arm with the highest mean, we're interested in this quantity \"on average\": in other words, our definition of regret will not be concerned with the randomness in the reward distributions themselves.\n",
    "\n",
    "$$ \\bar{R}_T= \\sum_{t=1}^T \\mu_{A^\\ast} - \\mu_{A_t}$$\n",
    "\n",
    "Even though we're ignoring the randomness in the reward distributions and only looking at the means, the pseudoregret $\\bar{R}_T$ is still random! This is because our decisions $A_t$ are dependent on what rewards we observed from 1 to $t-1$. \n",
    "\n",
    "It is often simpler to analyze the regret, which is the expectation of the pseudoregret:\n",
    "\n",
    "$$ R_T= T \\mu^* - \\mathbb{E}\\left[\\sum_{t=1}^T \\mu_{A_t}\\right]$$\n",
    "\n",
    "### Lab setup: \n",
    "\n",
    "In this lab, the MAB instances will have a set of arms numbered $0,1,...,K-1$. Each arm $a=0,1,...,K-1$ is associated with a Gaussian reward distribution with mean $\\mu_a$ and standard deviation of $\\sigma_a=1.5$. To be able to analyze the various algorithms, the optimal arm $A^\\ast$ will always be arm $0$, and its mean will always be $\\mu^\\ast=10$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b626eb0e-8681-4f32-85f6-61b425c72a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to initialize the parameters for the arms that we will be pulling from.\n",
    "\n",
    "# Mean reward for each arm. Arm 0 has the highest mean, but the algorithm doesn't know that yet.\n",
    "means=[10,9,8,7,6,4]\n",
    "\n",
    "# Variance of the reward for each arm.\n",
    "variance=1.5\n",
    "standard_deviations=[np.sqrt(variance) for arm in range(len(means))]\n",
    "\n",
    "# Initialize the interactive environment for pulling arms.\n",
    "bandit_env=BanditEnv(means,standard_deviations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74bff573-ff91-48d9-986a-63a11e840a18",
   "metadata": {},
   "source": [
    "# Part 1.  The Frequentist Approach: Upper Confidence Bounds (UCB) \n",
    "\n",
    "The first algorithm we will analyze is the frequentist take on multi-armed bandits, known as the Upper Confidence Bounds (UCB) algorithm. \n",
    "\n",
    "For each arm $a \\in \\{ 0,1,...,K-1\\}$, you keep track of:\n",
    "\n",
    "1. $T_a(t)$: the number of times arm $a$ has been pulled up to and including iteration $t$.\n",
    "2. $X_{a}^{(1)},...,X_{a}^{(T_a(t))}$: the samples you have received from arm $a$. Let $\\hat \\mu_{a,T_a(t)}$ be the mean of those samples: $\\hat \\mu_{a,T_a(t)} = \\frac{1}{T_a(t)}\\sum_{i=1}^{T_a(t)}X_{a}^{(i)}$\n",
    "\n",
    "Using this information, you compute an upper confidence bound, $C_a(T_a(t),\\delta)$ that encompasses the true mean $\\mu_a$ with probability at least $1-\\delta$, for some $\\delta \\in [0,1]$. $C_a(T_a(t),\\delta)$, must therefore satisfy:\n",
    "\n",
    "$$ \\mathbb{P}\\bigg(\\mu_a < \\hat \\mu_{a,T_a(t)} + C_a(T_a(t),\\delta)\\bigg) > 1 - \\delta.$$\n",
    "\n",
    "As an edge case, after $0$ samples, we simply set the upper bound on $\\mu_a$ to $\\infty$, since it's always true that $\\mathbb{P}(\\mu_a < \\infty) > 1 - \\delta$.\n",
    "\n",
    "The algorithm then pulls, at each round $t$, the arm with the highest upper confidence bound based on the results we saw up to time $t-1$:\n",
    "\n",
    "$$ A_t=\\underset{a \\in \\{ 0,1,...,K-1\\}}{\\operatorname{argmax}} \\hat\\mu_{a,T_a(t-1)} + C_a(T_a(t-1),\\delta).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9793f5-5b49-4eb1-8f73-06e115a67d31",
   "metadata": {},
   "source": [
    "## The UCB algorithm\n",
    "We will now implement the classic version of the UCB algorithm using the Hoeffding bound that we derived in lecture.\n",
    "\n",
    "In lecture, we used the Hoeffding bound for the sample mean of bounded random variables. Here instead we will use a related bound for the sample mean of normally distributed random variables. The sample mean of Gaussian random variables $X_{a}^{(1)},...,X_{a}^{(T_a(t))}$, $X_a^{(i)} \\sim \\mathcal{N}(\\mu_a, \\sigma_a)$, satisfies the following form of the Hoeffding Inequality:\n",
    "\n",
    "$$ P( \\hat \\mu_{a,T_a(t)}-\\mu_a \\leq -\\epsilon) \\leq \\exp\\left( -\\frac{T_a(t)\\epsilon^2}{2\\sigma_a^2}\\right).$$\n",
    "\n",
    "*Take a minute to compare this bound with the bounds discussed in lecture.*\n",
    "\n",
    "This bound results in the upper confidence bound on $\\mu_a$:\n",
    "\n",
    "$$P\\left(\\mu_a <\\hat \\mu_{a,T_a(t)} + \\sigma_a\\sqrt{\\frac{2\\log{1/\\delta}}{T_a(t)}}\\right)> 1-\\delta. $$\n",
    "\n",
    "where $\\hat\\mu_{a,T_a(t)}$ is the current sample mean for arm $a$:\n",
    "\n",
    "$$ \\hat\\mu_{a,T_a(t)} =\\frac{1}{T_a(t)} \\sum_{i=1}^{T_a(t)} X_{a}^{(i)}$$\n",
    "\n",
    "and the confidence bound term added to $\\hat\\mu_{a,T_a(t)}$ is:\n",
    "$$C_a(T_a(t),\\delta) = \\sigma_a\\sqrt{\\frac{2\\log{1/\\delta}}{T_a(t)}}.$$\n",
    "\n",
    "To handle the edge case where we've seen $0$ samples from arm $a$ so far (i.e. $T_a(t-1) = 0$), we set the upper bound on $\\mu_a$ to $\\infty$. Specifically, we set\n",
    "\n",
    "$$ C_a(T_a(t),\\delta) = \\begin{cases} \\infty \\  &\\text{ if } T_a(t)=0 \\\\  \\sigma_a\\sqrt{\\frac{2\\log{1/\\delta}}{T_a(t)}} \\ \\ \\ \\ \\ \\  &\\text{ if } T_a(t)>0\\end{cases}$$\n",
    "and \n",
    "$$ \\hat\\mu_{a,T_a(t)} = \\begin{cases} \\infty \\  &\\text{ if } T_a(t)=0 \\\\  \\frac{1}{T_a(t)} \\sum_{i=1}^{T_a(t)} X_{a}^{(i)} \\ \\ \\ \\ \\ \\  &\\text{ if } T_a(t)>0\\end{cases}$$\n",
    "\n",
    "Finally, as mentioned earlier, we choose the arm $A_t$ at time $t$ as follows:\n",
    "\n",
    "$$ A_t=\\underset{a \\in \\{ 0,1,...,K-1\\}}{\\operatorname{argmax}} \\hat\\mu_{a,T_a(t-1)} + C_a(T_a(t-1),\\delta).$$\n",
    "\n",
    "We will choose a $\\delta$ that decreases with time to ensure that we will explore the arms at first: \n",
    "$$ \\delta=\\frac{1}{t^3}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd82ae33-4839-4ef5-92a8-8fde6363dbda",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Question 1: \n",
    "\n",
    "Now, use this formula for $A_t$ to fill out the following function which returns the choice of arm as well as the upper confidence bounds of each arm. In the code below, we use the variable `c_bound` to refer to the entire term $\\hat\\mu_{a,T_a(t)} + C_a(T_a(t),\\delta)$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5720515a-f5fa-469c-8293-f654bcda92e1",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "def UCB_pull_arm(t, standard_deviations, times_pulled, rewards):\n",
    "    \"\"\" Implement the choice of arm for the UCB algorithm\n",
    "    \n",
    "    Inputs:\n",
    "        t : int, current iteration\n",
    "        standard_deviations : a list of length K (where K is the number of arms) of the \n",
    "            standard deviations associates with each arm\n",
    "        times_pulled: a list of length K (where K is the number of arms) of the number \n",
    "            of times each arm has been pulled.\n",
    "        rewards: a list of K lists. Each of the K lists holds the samples received from\n",
    "            pulling each arm up to iteration t. \n",
    "\n",
    "    Outputs:\n",
    "        arm: an integer representing the arm that the UCB algorithm would choose.\n",
    "        confidence_bounds: a list of the confidence bounds for each arm\n",
    "    \"\"\"\n",
    "\n",
    "    K = len(times_pulled)\n",
    "    delta = 1/(t**3)\n",
    "    \n",
    "    confidence_bounds=[]\n",
    "    for arm in range(K):\n",
    "        if times_pulled[arm]==0:\n",
    "            c_bound = ...\n",
    "        else:\n",
    "            c_bound = ...\n",
    "            \n",
    "        confidence_bounds.append(c_bound)\n",
    "            \n",
    "    arm = ...\n",
    "    \n",
    "    return arm, confidence_bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4fb222",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfca1fe7-a8da-461b-be29-86ecfcb22bf4",
   "metadata": {},
   "source": [
    "Here is a cell for testing with a simple example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5849414c-a155-4d2b-ac4a-4acb446ba012",
   "metadata": {},
   "outputs": [],
   "source": [
    "times_pulled_test = [3, 5, 7, 4, 0]\n",
    "t_test = np.sum(times_pulled_test) + 1\n",
    "standard_deviations_test = [0.4, 0.2, 0.1, 0.2, 0.3]\n",
    "rewards_test = [[10.4, 10.6, 11], \n",
    "                [8, 13, 12, 11, 9], \n",
    "                [9, 10, 10, 8, 9.5, 10.5, 11],\n",
    "                [8.3, 9.6, 7.9, 8.1],\n",
    "                []]\n",
    "test_arm, test_confidence_bounds = UCB_pull_arm(t_test, standard_deviations_test, times_pulled_test, rewards_test)\n",
    "\n",
    "print(test_arm)\n",
    "print(test_confidence_bounds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a18349f-4ba9-475c-9061-98d2c29c284f",
   "metadata": {},
   "source": [
    "### UCB Regret\n",
    "Given the function you have filled out, let us investigate the regret of the UCB algorithm.  Since the regret is an expectation of the pseudo-regret, the following cell runs the algorithm $20$ times and computes the average pseudo-regret across all runs at each time step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d587e2d-55cc-4821-9a9a-f23b33bea7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "#Initialize Figure\n",
    "plt.rcParams['figure.figsize']=[9,4]\n",
    "plt.figure()\n",
    "\n",
    "# Define the time horizon of each run, and the number of runs of each the algorithm.\n",
    "T=1000\n",
    "num_runs=20\n",
    "\n",
    "#Initialize pseudo-regret\n",
    "UCB_pseudo_regret=np.zeros([T, num_runs])\n",
    "for run in range(num_runs):\n",
    "    #Initialize Bandit_environment\n",
    "    bandit_env.initialize(make_plot=0)\n",
    "    for t in range(1,T+1):\n",
    "        #Choose arm using UCB algorithm\n",
    "        arm,confidence_bounds=UCB_pull_arm(t, standard_deviations, bandit_env.times_pulled,bandit_env.rewards)\n",
    "        \n",
    "        #Pull Arm\n",
    "        bandit_env.pull_arm(arm)\n",
    "        \n",
    "    #Keep track of pseudo-regret for each run\n",
    "    UCB_pseudo_regret[:, run] = np.array(bandit_env.regret[1:])\n",
    "\n",
    "#Make plot\n",
    "\n",
    "#First, plot the pseudoregret from each run so we can see how it varies\n",
    "plt.plot(UCB_pseudo_regret, color='tab:blue', alpha=0.3)\n",
    "\n",
    "#Then, plot our estimate of the regret (average pseudo-regret)\n",
    "UCB_regret = np.mean(UCB_pseudo_regret, axis=1)\n",
    "plt.plot(UCB_regret, color='black')\n",
    "\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Regret')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1358b7bc-b51d-47e6-b390-0a13ba97e167",
   "metadata": {},
   "source": [
    "# Part 2: The Bayesian Approach with Thompson Sampling \n",
    "Next, we will analyze the Bayesian take on multi-armed bandits, known as Thompson Sampling. In this setting, we would treat the mean as a *random* variable, instead of a fixed, hidden one. This allows to begin with a prior over the mean of each arm $\\mu_a \\sim \\pi_a$.\n",
    "\n",
    "In an ideal world, at each round $t=1,2...$, we would like an algorithm that computes the posterior probability $q_{a,t}$ that arm $a\\in \\mathcal{A}$ has the highest mean reward.\n",
    "\n",
    "$$q_{a,t}=\\mathbb{P}\\bigg(\\mu_a=\\max_{a'} \\mu_{a'} \\ \\bigg| \\ X_{1,A_1},...,X_{t-1,A_{t-1}}\\bigg).$$\n",
    "\n",
    "In this scenario, the choice of arm would then be randomly sampled from the distribution $q_t$ over $\\mathcal{A}$, where each arm $a\\in \\mathcal{A}$ has probability $q_{a,t}$:\n",
    "\n",
    "$$ A_t \\sim q_t $$ \n",
    "\n",
    "## Implementing Thompson Sampling\n",
    "\n",
    "However, in practice, the posterior distribution above is often intractable to compute. Instead, we'll implement a simpler algorithm (the one from lecture) that nevertheless accomplishes the same task.\n",
    "\n",
    "At each rount $t=1,2....$, you keep track of the posterior distribution over $\\mu_a$, for each arm $a \\in \\{ 0,1,...,K-1\\}$,  given all the samples you have observed from that arm $X_{a,1},...,X_{a,T_a(t-1)}$. Denote by $p_{a,t}$ the posterior distribution of the mean reward associated with arm $a$, after observing all the samples up to time $t$.\n",
    "\n",
    "$$p_{a,t}=\\mathbb{P}(\\mu_a | X_{a,1},...,X_{a,T_a(t-1)}).$$\n",
    "\n",
    "You then take one posterior sample from $p_{a,t}$ and choose the arm with the highest sample:\n",
    "\n",
    "1. $\\tilde\\mu_{a,t} \\sim  p_{a,t}$ for $a \\in \\{0,1,...,K-1\\}$.\n",
    "2. Choose arm:\n",
    "$$ A_t=\\underset{a \\in \\{ 0,1,...,K-1\\}}{\\operatorname{argmax}} \\tilde\\mu_{a,t}$$\n",
    "\n",
    "Since the reward distributions in this lab are Gaussians with known variance $\\sigma_a^2$, we know from our investigation of conjugate priors that if we have Gaussian priors: $\\mu_a\\sim\\mathcal{N}(\\mu_{a,0},\\sigma_{a,0}^2)$, and Gaussian likelihoods $X_a^{(i)}|\\mu_a \\sim \\mathcal{N}(\\mu_a, \\sigma_a)$ the posterior distribution for each arm will also be a Gaussian. \n",
    "\n",
    "Therefore, to implement Thompson Sampling in this lab, the posterior distributions for each arm in this lab at each time $t=1,2,...$ are given by:\n",
    "\n",
    "$$ p_{a,t}=\\mathcal{N}(\\hat\\mu_{a,t},\\hat{\\sigma}_{a,t}^2)$$\n",
    "\n",
    "where,\n",
    " $$\\hat{\\sigma}_{a,t}^2 =\\bigg(\\frac{1}{\\sigma_{a,0}^2}+\\frac{T_a(t-1)}{\\sigma_a^2}\\bigg)^{-1} $$\n",
    "$$ \\hat\\mu_{a,t}=\\hat{\\sigma}_{a,t}^2 \\bigg( \\frac{\\mu_{a,0}}{\\sigma_{a,0}^2}+\\frac{\\sum_{i=1}^{T_a(t-1)} X_{a,i}}{\\sigma_a^2} \\bigg)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc7abea-1072-47da-9a33-77041848a5ac",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Question 2b:\n",
    "\n",
    "Fill out the following function that implements the choice of arm for the Thompson Sampling algorithm with Gaussian arms and prior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b84fd4-65b5-4e34-85c4-ebe76f1e5cbc",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "def TS_pull_arm(t,variances,times_pulled,rewards,prior_means,prior_variances):\n",
    "    \"\"\" \n",
    "    Implement the choice of arm for the Thompson Sampling Algorithm when the arms and priors are Gaussians.\n",
    "    \n",
    "    Inputs:\n",
    "        t: int, number of iteration of the bandit algorithm.\n",
    "        variances: a list of length K (where K is the number of arms) of the variances\n",
    "            corresponding to each arm (\\sigma_a^2 in the likelihood expression above)\n",
    "        times_pulled: a list of length K of the number of times each arm has been pulled.\n",
    "        rewards: a list of K lists. Each of the K lists holds the samples received from pulling each arm up \n",
    "            to iteration t.\n",
    "        prior_means: a list of length K with the mean of the priorsfor each arm.\n",
    "        prior_mea: a list of length K with the variance of the prior for each arm.\n",
    "    \n",
    "    Outputs:\n",
    "        arm: integer representing the arm that the TS algorithm would choose.\n",
    "        posterior_samples: list of samples from the posterior used to choose the arm. \n",
    "        posterior_means: list of means of the posterior for each arm\n",
    "        posterior_vars: list of variances of the posteriors of each arm\n",
    "    \"\"\"\n",
    "\n",
    "    K=len(times_pulled)\n",
    "    \n",
    "    posterior_samples=[]\n",
    "    posterior_means=[]\n",
    "    posterior_vars=[]\n",
    "    for arm in range(K):\n",
    "        \n",
    "        # TODO: fill in arm_var_hat, which is \\hat\\sigma^2_{a,t}.\n",
    "        # Hint: \\hat\\sigma^2_{a,0} is prior_variances[arm], \\sigma^2_a is variance,\n",
    "        arm_var_hat = ...\n",
    "        \n",
    "        # TODO: fill in mean_hat, which is \\hat\\mu_{a,t}.\n",
    "        # Hint: \\mu_{a,0} is prior_means[arm], and X_a^{(i)} is rewards[arm] (as before).\n",
    "        mean_hat = ...\n",
    "        \n",
    "        posterior_samples.append(np.random.normal(mean_hat,arm_var_hat))\n",
    "        posterior_means.append(mean_hat)\n",
    "        posterior_vars.append(arm_var_hat)\n",
    "            \n",
    "    arm = ...\n",
    "    \n",
    "    return arm, posterior_samples, posterior_means, posterior_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c5c2729",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8f294c-246e-4ddf-aebf-4f9c04f3d854",
   "metadata": {},
   "source": [
    "Here is a cell for testing with a simple example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7356e5d-d031-4fd3-bc09-d4bd5b052300",
   "metadata": {},
   "outputs": [],
   "source": [
    "times_pulled_test = [3, 5, 7, 4, 1]\n",
    "prior_means_test=[8,5,7,9,6]\n",
    "prior_variances_test=[2.5, 0.1, 1.6, 2.3, 1.7]\n",
    "t_test = np.sum(times_pulled_test) + 1\n",
    "variances_test = [0.4, 0.2, 0.1, 0.2, 0.5]\n",
    "rewards_test = [[10.4, 12.6, 11], \n",
    "                [8, 13, 12, 11, 9], \n",
    "                [9, 10, 10, 8, 9.5, 10.5, 11],\n",
    "                [8.3, 9.6, 7.9, 8.1],\n",
    "                [8]]\n",
    "test_arm, posterior_samples_test, posterior_means_test, posterior_vars_test = TS_pull_arm(t_test, \n",
    "                                                                                          variances_test, \n",
    "                                                                                          times_pulled_test, \n",
    "                                                                                          rewards_test,\n",
    "                                                                                          prior_means_test,\n",
    "                                                                                          prior_variances_test)\n",
    "\n",
    "\n",
    "print(\"means:\")\n",
    "print(posterior_means_test)\n",
    "print(\"variances:\")\n",
    "print(posterior_vars_test)\n",
    "print(\"samples:\")\n",
    "print(posterior_samples_test)\n",
    "print(\"arm pulled: \" + str(test_arm))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a3562a0-c13b-422d-8f16-3044683d9a01",
   "metadata": {},
   "source": [
    "## Thompson Sampling with Good Priors\n",
    "As we saw in class, the performance of Thompson Sampling can vary drastically with the quality of the prior. \n",
    "\n",
    "First, let us analyze the performance of Thompson Sampling when the priors reflect the correct rankings of the arms (meaning that the prior mean for arm $0$ is the highest). We will compare it to the performance of the UCB algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b3b164-fba6-40d5-83cd-3c0fda3164ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# No TODOs here, just run the code and inspect the plot\n",
    "#Initialize Figure\n",
    "plt.rcParams['figure.figsize']=[9,4]\n",
    "plt.figure()\n",
    "\n",
    "# Variance of the reward for each arm.\n",
    "variance=1.5\n",
    "true_variances=[variance for arm in range(len(means))]\n",
    "\n",
    "#Define Prior Means and Variances\n",
    "prior_means=[12,9,8,7,4,3]\n",
    "prior_vars=[3.2,3.2,3.2,3.2,3.2,3.2]\n",
    "\n",
    "\n",
    "#Initialize pseudo-regret\n",
    "TS_pseudo_regret=np.zeros([T, num_runs])\n",
    "for run in range(num_runs):\n",
    "    #Initialize bandit environment\n",
    "    bandit_env.initialize(make_plot=0)\n",
    "    for t in range(1,T+1):\n",
    "        #Choose arm with Thompson Sampling\n",
    "        arm,samples,means,variances=TS_pull_arm(t,true_variances,bandit_env.times_pulled,bandit_env.rewards,prior_means,prior_vars)\n",
    "        \n",
    "        #Pull Arm\n",
    "        bandit_env.pull_arm(arm)\n",
    "    \n",
    "    #Keep track of regret Regret\n",
    "    TS_pseudo_regret[:, run] = np.array(bandit_env.regret[1:])\n",
    "\n",
    "#Make plot\n",
    "\n",
    "#First, plot the UCB regret and pseudo-regret\n",
    "plt.plot(UCB_pseudo_regret, color='tab:blue', alpha=0.3)\n",
    "plt.plot(UCB_regret, color='blue', label='UCB Regret')\n",
    "\n",
    "#Then, plot Thompson Sampling regret and pseudo-regret\n",
    "plt.plot(TS_pseudo_regret, color='tab:red', alpha=0.1)\n",
    "TS_regret = np.mean(TS_pseudo_regret, axis=1)\n",
    "plt.plot(TS_regret, color='red', label='TS Regret')\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Regret')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb563ec-b68a-4b51-bc2d-b8b0c2208484",
   "metadata": {},
   "source": [
    "## Thompson Sampling with Bad Priors\n",
    "Now let us analyze the performance of Thompson Sampling when the priors have completely incorrect correct rankings of the arms, meaning that the prior mean for arm $0$ is the lowest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ffdf25e-01e5-4edf-a368-1878b3636e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# No TODOs here, just run the code and inspect the plot\n",
    "#Initialize Figure\n",
    "plt.rcParams['figure.figsize']=[9,4]\n",
    "plt.figure()\n",
    "\n",
    "#Define prior means and standard deviations\n",
    "prior_means=[2,3,4,5,6,7]\n",
    "prior_vars=[3.2,3.2,3.2,3.2,3.2,3.2]\n",
    "\n",
    "#Initialize pseudo-regret\n",
    "TS_pseudo_regret=np.zeros([T, num_runs])\n",
    "for run in range(num_runs):\n",
    "    #Initialize bandit environment\n",
    "    bandit_env.initialize(make_plot=0)\n",
    "    for t in range(1,T+1):\n",
    "        #Choose arm with Thompson Sampling\n",
    "        arm,samples,means,variances=TS_pull_arm(t,true_variances,bandit_env.times_pulled,bandit_env.rewards,prior_means,prior_vars)\n",
    "        \n",
    "        #Pull Arm\n",
    "        bandit_env.pull_arm(arm)\n",
    "    \n",
    "    #Keep track of regret Regret\n",
    "    TS_pseudo_regret[:, run] = np.array(bandit_env.regret[1:])\n",
    "\n",
    "#Make plot\n",
    "\n",
    "#First, plot the UCB regret and pseudo-regret\n",
    "plt.plot(UCB_pseudo_regret, color='tab:blue', alpha=0.3)\n",
    "plt.plot(UCB_regret, color='blue', label='UCB Regret')\n",
    "\n",
    "#Then, plot Thompson Sampling regret and pseudo-regret\n",
    "plt.plot(TS_pseudo_regret, color='tab:red', alpha=0.1)\n",
    "TS_regret = np.mean(TS_pseudo_regret, axis=1)\n",
    "plt.plot(TS_regret, color='red', label='TS Regret')\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel('Time')\n",
    "plt.ylim(-10, max(TS_regret) + 10)\n",
    "plt.ylabel('Regret')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca36aac-3d90-4419-acf6-6a9dcf1f9f74",
   "metadata": {},
   "source": [
    "## Thompson Sampling with the same prior for each arm \n",
    "Now let us analyze the performance of Thompson Sampling when the priors are the same for all arms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8802a801-3290-46a0-8a26-7b855ab99068",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# No TODOs here, just run the code and inspect the plot\n",
    "plt.rcParams['figure.figsize']=[9,4]\n",
    "plt.figure()\n",
    "\n",
    "#Define prior means and variances\n",
    "prior_means=[8,8,8,8,8,8]\n",
    "prior_vars=[2.5,2.5,2.5,2.5,2.5,2.5]\n",
    "\n",
    "#Initialize pseudo-regret\n",
    "TS_pseudo_regret=np.zeros([T, num_runs])\n",
    "for run in range(num_runs):\n",
    "    #Initialize bandit environment\n",
    "    bandit_env.initialize(make_plot=0)\n",
    "    for t in range(1,T+1):\n",
    "        #Choose arm with Thompson Sampling\n",
    "        arm,samples,means,variances=TS_pull_arm(t,true_variances,bandit_env.times_pulled,bandit_env.rewards,prior_means,prior_vars)\n",
    "        \n",
    "        #Pull Arm\n",
    "        bandit_env.pull_arm(arm)\n",
    "    \n",
    "    #Keep track of regret Regret\n",
    "    TS_pseudo_regret[:, run] = np.array(bandit_env.regret[1:])\n",
    "\n",
    "#Make plot\n",
    "\n",
    "#First, plot the UCB regret and pseudo-regret\n",
    "plt.plot(UCB_pseudo_regret, color='tab:blue', alpha=0.3)\n",
    "plt.plot(UCB_regret, color='blue', label='UCB Regret')\n",
    "\n",
    "#Then, plot Thompson Sampling regret and pseudo-regret\n",
    "plt.plot(TS_pseudo_regret, color='tab:red', alpha=0.1)\n",
    "TS_regret = np.mean(TS_pseudo_regret, axis=1)\n",
    "plt.plot(TS_regret, color='red', label='TS Regret')\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Regret')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f1f164-3268-483f-be8b-1c2f8c23c8e5",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "## Question 3: Pros and Cons of UCB and Thompson Sampling\n",
    "\n",
    "In the following cell, write a few sentences comparing and contrasting UCB and Thompson Sampling. What are some pros and cons of UCB and of Thompson Sampling?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae66e89f",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75b9432-6421-4cc4-ab38-a6e094c90f1c",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "## Congratulations! You have finished Lab 10!\n",
    "\n",
    "Below, you will see two cells. Running the first cell will automatically generate a PDF of all questions that need to be manually graded, and running the second cell will automatically generate a zip with your autograded answers. **You are responsible for both the coding portion (the zip from Lab 10) and the written portion (the PDF of written responses from Lab 10) to their respective Gradescope portals.** The coding proportion should be submitted to the `Lab 10` assignment as a single zip file, and the written portion should be submitted to `Lab 10 PDF` assignment as a single pdf file. When submitting the written portion, please ensure you select pages appropriately.\n",
    "\n",
    "If there are issues with automatically generating the PDF in the first cell, you can try downloading the notebook as a PDF by clicking on `File -> Save and Export Notebook As... -> PDF`. If that doesn't work either, you can manually take screenshots of your answers to the manually graded questions and submit those. Either way, **you are responsible for ensuring your submission follows our requirements, we will NOT be granting regrade requests for submissions that don't follow instructions.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415cee86-f0ce-4777-a4ec-c1a7f791273b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.image as mpimg\n",
    "from otter.export import export_notebook\n",
    "from os import path\n",
    "from IPython.display import display, HTML\n",
    "export_notebook(\"lab10.ipynb\", filtering=True, pagebreaks=True)\n",
    "if(path.exists('lab10.pdf')):\n",
    "    img = mpimg.imread('its_arms.jpeg')\n",
    "    imgplot = plt.imshow(img)\n",
    "    imgplot.axes.get_xaxis().set_visible(False)\n",
    "    imgplot.axes.get_yaxis().set_visible(False)\n",
    "    plt.show()\n",
    "    display(HTML(\"Download your PDF <a href='lab10.pdf' download>here</a>.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96f4f59",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "---\n",
    "\n",
    "To double-check your work, the cell below will rerun all of the autograder tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ed3516",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f559a1",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Submission\n",
    "\n",
    "Make sure you have run all cells in your notebook in order before running the cell below, so that all images/graphs appear in the output. The cell below will generate a zip file for you to submit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304df62e",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.export(pdf=False, force_save=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b56ab8",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "otter": {
   "OK_FORMAT": true,
   "tests": {
    "q1": {
     "name": "q1",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> K_test = 5\n>>> times_pulled_test = [3, 5, 7, 4, 0]\n>>> t_test = np.sum(times_pulled_test) + 1\n>>> standard_deviations_test = [0.4, 0.2, 0.1, 0.2, 0.5]\n>>> rewards_test = [[10.4, 10.6, 11], [8, 13, 12, 11, 9], [9, 10, 10, 8, 9.5, 10.5, 11], [8.3, 9.6, 7.9, 8.1], []]\n>>> test_arm, test_confidence_bounds = UCB_pull_arm(t_test, standard_deviations_test, times_pulled_test, rewards_test)\n>>> checks = [test_arm == 4, np.isinf(test_confidence_bounds[-1])]\n>>> opt_vals = [11.64, 10.98, 9.87, 8.9]\n>>> for a in range(K_test - 1):\n...     checks.append(np.abs(opt_vals[a] - test_confidence_bounds[a]) <= 0.1)\n>>> assert np.all(checks)\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q2": {
     "name": "q2",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> K_test = 5\n>>> times_pulled_test = [3, 5, 7, 4, 1]\n>>> prior_means_test = [8, 5, 7, 9, 6]\n>>> prior_variances_test = [2.5, 0.1, 1.6, 2.3, 1.7]\n>>> t_test = np.sum(times_pulled_test) + 1\n>>> variances_test = [0.4, 0.2, 0.1, 0.2, 0.5]\n>>> rewards_test = [[10.4, 12.6, 11], [8, 13, 12, 11, 9], [9, 10, 10, 8, 9.5, 10.5, 11], [8.3, 9.6, 7.9, 8.1], [8]]\n>>> test_arm, posterior_samples_test, posterior_means_test, posterior_vars_test = TS_pull_arm(t_test, variances_test, times_pulled_test, rewards_test, prior_means_test, prior_variances_test)\n>>> checks = [test_arm == 0]\n>>> opt_vals_means = [11.16, 9.0, 9.69, 8.49, 7.55]\n>>> opt_vals_vars = [0.123, 0.0286, 0.014, 0.049, 0.386]\n>>> for a in range(K_test):\n...     checks.append(np.abs(opt_vals_means[a] - posterior_means_test[a]) <= 0.1)\n...     checks.append(np.abs(opt_vals_vars[a] - posterior_vars_test[a]) <= 0.01)\n>>> assert np.all(checks)\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
